\documentclass[12pt,letter]{article}

%% \usepackage[fleqn]{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsfonts,amsthm,bm}
\usepackage{breqn}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{algorithm2e}
\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{subcaption}
%% \usepackage{datetime}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{mathrsfs}
\usepackage{fancyhdr}
\usepackage{fancyvrb}
\usepackage{parskip} %turns off paragraph indent
\usepackage{float}
\usepackage{empheq}
\usepackage{enumitem}

\pagestyle{fancy}

\usetikzlibrary{arrows}

\DeclareMathOperator*{\argmin}{argmin}
\newcommand*{\argminl}{\argmin\limits}

\DeclareMathOperator*{\argmax}{argmax}
\newcommand*{\argmaxl}{\argmax\limits}

\newcommand{\mathleft}{\@fleqntrue\@mathmargin0pt}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\ppartial}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\notimplies}{\;\not\!\!\!\implies}

\setcounter{MaxMatrixCols}{20}

\begin {document}

  % \begin{cases}
  %     0, & \text{if}\ a=1 \\
  %     1, & \text{otherwise}
  %   \end{cases}

\lhead{Convex Optimization - HW4}
\rhead{(Bill) Yuan Liu, 996954078, 2020/03/20}

\begin{enumerate}
  \item SDP Relaxation and Heuristics for Two-Way Partitioning Problem
  \begin{enumerate}
  \item Q 5.39 textbook\\
    \begin{align*}
      min\ x^T W x\\
      s.t.\ x_i^2 = 1, \forall i \in \{1,..,n\}\\
    \end{align*}
    \begin{enumerate}
    \item
    Show that the two-way partitioning problem can be cast as
    \begin{align*}
      min\ tr(WX)\\
      s.t.\ X \succeq 0, rank(X)=1\\
      X_{ii}=1, \forall i \in \{1,..,n\}
    \end{align*}

    \begin{align*}
      &x^T W x = tr(x^T W x)=tr(Wxx^T)\\
      &let\ X=xx^T\\
      &(\forall i) x_i^2 = 1 \iff x_i = \{-1,1\} \implies x^TIx = n\\
      &x^TIx = tr(xx^T)=n\\
      &(\forall i,j) X_{ij} = \{-1,1\}\\
      &((\exists i)X_{ii}=-1 \implies tr(X) < n)\\
      &thus, for\ tr(X)=n: (\forall i)X_{ii}=1\\
      \\
      &X=xx^T=x
      \begin{bmatrix}
        a_1 & a_2 & .. & a_n
      \end{bmatrix} =
                         \begin{bmatrix}
                           a_1 x & a_2 x & .. & a_n x
                         \end{bmatrix}, a_i \in \R, x \in \R^n\\
      &(\forall i)(\exists j) \beta_{ij} a_i x = a_j x \implies \beta_{ij} a_i x - a_j x = 0\\
      &let\ \gamma_{ij} = \beta_{ij} a_i - a_j\\
      &\gamma_{ij} x = 0\\
      &x\not=0 \implies ((\forall i)(\exists j) \gamma_{ij} = 0 \implies linear\ dependence\ between\ all\ column\ vectors\ of\ X)\\
      &thus,\ rank(X)=1\\
      \\
      &(\forall w) w^TXw = w^Txx^Tw = (x^Tw)^T x^Tw\\
      &(\forall i, w)(x^Tw)_i (x^Tw)_i \geq 0 \implies (\forall w)(x^Tw)^T (x^Tw) \geq 0 \iff X\ is\ SPD
    \end{align*}
    Combining all constraints and objective forms the desired result.
    \pagebreak
  \item
    SDP relaxation of two-way partitioning problem. Using the formulation in part (a), we can form the relaxation:
    \begin{align*}
      min\ tr(W X)\\
      s.t.\ X \succeq 0\\
      X_{ii} = 1, \forall i \in \{1,..,n\}
    \end{align*}
    This problem is an SDP, and therefore can be solved efficiently. Explain why its optimal value gives a lower bound on the optimal value of the two-way partitioning problem (5.113). What can you say if an optimal point $X^*$ for this SDP has rank one?
    \begin{align*}
      &L(X,Z,v) = tr(WX) - tr(XZ) + tr(diag(v)diag(X)-I)\\
      &L(X,Z,v) = -tr(diag(v)I) + tr(WX) - tr(XZ) + tr(diag(v)diag(X))\\
      &g(Z,V) =
        \begin{cases}
          -1^T v ,& W - Z + diag(v) \succeq 0\\
          -\infty ,& o/w
        \end{cases}\\
      &dual\ problem:\\
      &\max_{Z,v} -1^T v\\
      &s.t.\ W - Z + diag(v) \succeq 0\\
      &Z \succeq 0
    \end{align*}
    % Dual of original problem (5.114):
    % \begin{align*}
    %   &maximize\ -1^T v\\
    %   &s.t.\ W + diag(v) \succeq 0
    % \end{align*}
    % It is evident that solution to the dual of relaxed problem has a tightened inequality constraint due to $Z$.
    % \begin{align*}
    %   &W + diag(v) \succeq Z\\
    % \end{align*}
    % This leads to potentially bigger $v$ and hence potentially larger objective value to dual maximization problem (smaller objective value to the primal minimization problem). Thus, solution of relaxed problem provides a lower bound to the the original problem.\\
    
    % \pagebreak
    
    If an optimal point $X^*$ for the relaxed problem has rank one:\\
    $X^*$ has minimal possible rank and $X^*\not=0$, $X^*\succeq 0$.\\
    $X^*\succeq 0$, so primal feasible.\\
    Functions are all differentiable, KKT conditions apply at optimality where there exists a dual solution.\\
    Dual of relaxed problem is feasible: $W + diag(v) \succeq Z, Z\succeq 0$\\
    Using complementary slackness: $X^*\neq 0, -tr(X^*Z^*) = 0 \implies Z^* = 0$.\\
    
    Dual problem of relaxed problem at optimality:
    \begin{align*}
      &\max_{v,Z} -1^T v = [\max_{v}\ -1^T v]_{Z=Z^*}\\
      &s.t.\ W + diag(v) \succeq Z^*, Z^*=0 \implies \\
      &s.t.\ W + diag(v) \succeq 0
    \end{align*}
    This solution is equivalent to the solution of the dual of the original problem. Thus, if $rank(X^*)=1$ of the relaxed problem, $X^*$ obtains the same solution as the original problem where $x^*x^*^T=X^*$ as required.
    \pagebreak
  \item
    We now have two SDPs that give a lower bound on the optimal value of the two-way partitioning problem (5.113): the SDP relaxation (5.115) found in part (b), and the Lagrange dual of the two-way partitioning problem, given in (5.114). What is the relation between the two SDPs? What can you say about the lower bounds found by them? Hint: Relate the two SDPs via duality.
    \begin{align*}
      (5.115)\ &min\ tr(W X)\\
               &s.t.\ X \succeq 0\\
               &X_{ii} = 1, \forall i \in \{1,..,n\}
    \end{align*}
    \begin{align*}
      (5.114)\ &maximize\ -1^T v\\
               &s.t.\ W + diag(v) \succeq 0
    \end{align*}    
    % \pagebreak
    Taken from previous section, dual of relaxed problem:
    \begin{align*}
      &L(X,Z,v) = tr(WX) - tr(XZ) + tr(diag(v)diag(X)-I)\\
      &L(X,Z,v) = -tr(diag(v)I) + tr(WX) - tr(XZ) + tr(diag(v)diag(X))\\
      &g(Z,V) =
        \begin{cases}
          -1^T v ,& W - Z + diag(v) \succeq 0\\
          -\infty ,& o/w
        \end{cases}
    \end{align*}
    Dual problem:
    \begin{align*}
      &\max_{Z,v} -1^T v\\
      &s.t.\ W - Z + diag(v) \succeq 0\\
        &Z \succeq 0
      \end{align*}

      It is evident that solution to the dual of relaxed problem has a tightened generalized inequality constraint due to $Z$ compared to (5.114):
      \begin{align*}
        &W + diag(v) \succeq Z
      \end{align*}
      This leads to potentially bigger $v$ and hence potentially larger objective value of dual maximization problem (smaller objective value to the primal minimization problem). Thus, solution of relaxed problem provides a lower bound to the the original problem.\\
    \end{enumerate}
    
  \pagebreak
  
\item Q 11.23(b-d) textbook\\
  
  \begin{tabular}{|c|c|c|c|c|}\hline
          & SDP bound (11.66)& Optimum & b (11.67)& c \\ \hline
    small & 5.33445288482 & 5.33440509588 & 12.30891878 & 5.33440509588\\ \hline
    medium & -42.2266162135 & -38.3691372273  & -13.05483539 & -38.3691372273\\ \hline
    large & 66.0855132055 & x & 2135.80923688 & 457.832259292\\ \hline
  \end{tabular}

    \begin{tabular}{|c|c|c|c|}\hline
      & d-a & d-b & d-c\\ \hline
      small & 5.33440509588 & 12.3089187826 &5.33440509588\\ \hline
      medium & -38.3691372273& -16.4179991404 &-38.3691372273 \\ \hline
      large & 1053.83794696& 1401.39879583 &424.105035811\\ \hline
    \end{tabular}\\

    Comparison of heuristic parition (b) to SDP bound:\\
    All obtained objetive values are within the expected range given by SDP lower bound, they do not close to the lower bound.\\

    Comparison of randomized method (c) to SDP bound:\\
    All obtained objective values are within the expected range given by SDP lower bound, and they are all closer to the SDP bound compared to (b)\\

    100 random samples + greedy single coordinate search (d-a):\\
    The obtained objective values work well for small to medium dimension sized problems since it effetively cover a majority of permutations possible, but becomes less effective for large problems due to exponential dimensionality explosion.\\
    
    Single heuristic solution + greedy single coordinate search (d-b):\\
    As expected, the obtained objective values are equivalent or better than Heuristic parition (b) alone, but is dependent on heuristic solution as the algorithm is greedy. Local search is show to have bigger marginal impact for higher dimensional problem.

    Randomized method + greedy single coordinate search (d-c):\\
    As expected, the obtained objective values are equivalent or better than randomized method (c) alone. Local search is show to have bigger marginal impact for higher dimensional problem. Local search plus randomized method seems to give a good balance in practice.
    
    \pagebreak

    Optimum (exhaustive):
\begin{verbatim}
import cvxpy as cp
import numpy as np
from scipy.io import loadmat
import numpy.linalg as linalg
import math
import copy

def gen_seq_exhaustive(dim):
    
    q = [[1], [-1]]

    while len(q[0]) < dim:
        qq =[]
        for i in q:
            a = copy.deepcopy(i)
            b = copy.deepcopy(i)
            a.append(1)
            b.append(-1)
            s = len(i)
            assert(len(a)==len(b))
            assert(len(a)==s+1)
            qq.append(a)
            qq.append(b)
        q = qq

    ret = np.zeros((len(q), dim))
    idx = 0
    for i in q:
        ret[idx,:] = i
        idx += 1

    return ret
\end{verbatim}
    \pagebreak
\begin{verbatim}
def solve_d_exhaustive(W):
    dim = W.shape[0]
    samples = gen_seq_exhaustive(dim)
    best_val = math.inf
    best_x = None
    for i in range(0, samples.shape[0]):
        x = samples[i,:].T
        v = (x.T).dot(W).dot(x)
        if v < best_val:
            best_val = v
            best_x = x
            
    print("problem size:", W.shape[0])
    print("best_objective (exhaustive): ", best_val)
    print("-----")
    return best_val, best_x
    
m = loadmat('../data/hw4data.mat')
w5 = np.array(m['W5'])
w10 = np.array(m['W10'])

solve_d_exhaustive(w5)
solve_d_exhaustive(w10)
\end{verbatim}

    
\pagebreak
    \begin{itemize}
    \item b) heuristic partitioning
\begin{verbatim}
def solve(W):
    print("problem size:", W.shape[0])
    
    #dual of original:
    print("dual of original:")
    dim = W.shape[0]
    v = cp.Variable((dim,1))
    constraints = [W + cp.diag(v) >> 0]
    prob = cp.Problem(cp.Maximize( -cp.sum(v) ),
                      constraints)
    prob.solve()

    print("prob.status:", prob.status)
    
    lower_bound = 0
    
    if prob.status not in ["infeasible", "unbounded"]:
        print("Optimal value: %s" % prob.value)
        lower_bound = prob.value

    print("lower_bound:", lower_bound)

    #dual of relaxed:
    print("dual of relaxed:")
    X = cp.Variable((dim,dim))
    constraints = [X >> 0, cp.diag(X) == np.ones((dim,))]
    prob = cp.Problem(cp.Minimize( cp.trace(cp.matmul(W,X)) ),
                      constraints)
    prob.solve()

    print("prob.status:", prob.status)
    
    if prob.status not in ["infeasible", "unbounded"]:
        print("Optimal value: %s" % prob.value)

    ret = prob.variables()[0].value
    eigenValues, eigenVectors = linalg.eig(ret)

    idx = eigenValues.argsort()[::-1]
    eigenValues = eigenValues[idx]
    eigenVectors = eigenVectors[:,idx]
    x_approx = np.sign(eigenVectors[0])[:,np.newaxis]
    p_heuristic = (x_approx.T).dot(W).dot(x_approx)
    print("heuristic objective: ", p_heuristic)
    print("-----")
    
m = loadmat('../data/hw4data.mat')
w5 = np.array(m['W5'])
w10 = np.array(m['W10'])
w50 = np.array(m['W50'])

solve(w5)
solve(w10)
solve(w50)
\end{verbatim}
      \pagebreak
    \item c) randomized method
\begin{verbatim}
import cvxpy as cp
import numpy as np
from scipy.io import loadmat
import numpy.linalg as linalg
import math

def solve(W):
    print("problem size:", W.shape[0])
    
    #dual of original:
    print("dual of original:")
    dim = W.shape[0]
    v = cp.Variable((dim,1))
    constraints = [W + cp.diag(v) >> 0]
    prob = cp.Problem(cp.Maximize( -cp.sum(v) ),
                      constraints)
    prob.solve()
    
    lower_bound = 0
    if prob.status not in ["infeasible", "unbounded"]:
        print("Optimal value: %s" % prob.value)
        lower_bound = prob.value

    print("lower_bound: ", lower_bound)

    #dual of relaxed:

    X = cp.Variable((dim,dim))
    constraints = [X >> 0, cp.diag(X) == np.ones((dim,))]
    prob = cp.Problem(cp.Minimize( cp.trace(cp.matmul(W,X)) ),
                      constraints)
    prob.solve(solver=cp.SCS, max_iters=4000,
               eps=1e-11, warm_start=True)
    
    if prob.status not in ["infeasible", "unbounded"]:
        print("Optimal value: %s" % prob.value)
    
    ret = prob.variables()[0].value

    K = 100 #number of samples
    xs_approx = np.random.multivariate_normal(np.zeros((dim,)),
                                              ret, size=(K))
    xs_approx = np.sign(xs_approx) #shape: (K,dim)

    p_best = math.inf
    for i in range(0,K):
        p = (xs_approx[i,:].dot(W)).dot(xs_approx[i,:].T)
        if p < p_best:
            p_best = p

    print("best objective (randomized): ", p_best, "size: ", K)
    print("-----")

    return p_best, xs_approx
    
m = loadmat('../data/hw4data.mat')
w5 = np.array(m['W5'])
w10 = np.array(m['W10'])
w50 = np.array(m['W50'])

solve(w5)
solve(w10)
solve(w50)
\end{verbatim}
      \pagebreak
    \item d) greedy heuristic refinement\\
      
      d-a (100 randomized initial points, single coordinate greedy search):
\begin{verbatim}
#generate samples each having dim numbers in {-1,1}
def gen_seq(dim, samples):
    return np.sign(np.random.rand(samples, dim) - 0.5)
    
def greedy(x,W):
    xx = x
    val = (xx.T).dot(W).dot(xx)

    while True:
        idx = None
        for i in range(0, xx.size):
            y = xx
            y[i] = -y[i]
            v = (y.T).dot(W).dot(y)
            if v < val:
                val = v
                idx = i
        if idx is None:
            break
        else:
            xx[i] = -xx[i]
    return val, xx

def solve_d_a(W):
    dim = W.shape[0]
    K = 100
    samples = gen_seq(dim, K)
    best_val = math.inf
    best_x = None
    for i in range(0,samples.shape[0]):
        x = samples[i,:].T
        val, xx = greedy(x, W)
        if val < best_val:
            best_val = val
            best_x = xx
            
    print("problem size:", W.shape[0])
    print("best_objective: ", best_val)
    print("-----")
    return best_val, best_x
    
m = loadmat('../data/hw4data.mat')
w5 = np.array(m['W5'])
w10 = np.array(m['W10'])
w50 = np.array(m['W50'])

solve_d_a(w5)
solve_d_a(w10)
solve_d_a(w50)
\end{verbatim}

      \pagebreak
      
      d-b (a heuristic solution + single coordinate greedy search):

\begin{verbatim}
def solve_heur(W):
    
    #dual of original:
    dim = W.shape[0]
    v = cp.Variable((dim,1))
    constraints = [W + cp.diag(v) >> 0]
    prob = cp.Problem(cp.Maximize( -cp.sum(v) ),
                      constraints)
    prob.solve()
    
    lower_bound = 0
    if prob.status not in ["infeasible", "unbounded"]:
        print("Optimal value: %s" % prob.value)
        lower_bound = prob.value

    print("lower_bound:", lower_bound)

    #dual of relaxed:
    X = cp.Variable((dim,dim))
    constraints = [X >> 0, cp.diag(X) == np.ones((dim,))]
    prob = cp.Problem(cp.Minimize( cp.trace(cp.matmul(W,X)) ),
                      constraints)
    prob.solve()
    
    if prob.status not in ["infeasible", "unbounded"]:
        print("Optimal value: %s" % prob.value)

    ret = prob.variables()[0].value
    eigenValues, eigenVectors = linalg.eig(ret)

    idx = eigenValues.argsort()[::-1]
    eigenValues = eigenValues[idx]
    eigenVectors = eigenVectors[:,idx]
    
    x_approx = np.sign(eigenVectors[0])[:,np.newaxis]
    p_heuristic = (x_approx.T).dot(W).dot(x_approx)
    print("heuristic objective: ", p_heuristic)

    return p_heuristic, x_approx.T
    
def greedy(x,W):
    xx = x
    val = (xx.T).dot(W).dot(xx)
    while True:
        idx = None
        for i in range(0, xx.size):
            y = xx
            y[i] = -y[i]
            v = (y.T).dot(W).dot(y)
            if v < val:
                val = v
                idx = i
        if idx is None:
            break
        else:
            xx[i] = -xx[i]
    return val, xx

def solve_d_b(W):
    
    print("performing greedy search")
    dim = W.shape[0]
    best_val = math.inf
    best_x = None
    _, xs = solve_heur(W)
    
    for i in range(0,xs.shape[0]):
        val, xx = greedy(xs[i,:].T, W)
        if val < best_val:
            best_val = val
            best_x = xx
            
    print("problem size:", W.shape[0])
    print("best objective (heuristic+greedy): ", best_val )
    print("-----")
    return best_val, best_x

m = loadmat('../data/hw4data.mat')
w5 = np.array(m['W5'])
w10 = np.array(m['W10'])
w50 = np.array(m['W50'])

solve_d_b(w5)
solve_d_b(w10)
solve_d_b(w50)
\end{verbatim}
      
      \pagebreak
      
      d-c (100 samples from randomized method + single coordinate greedy search):

\begin{verbatim}
def solve_rand(W):  
    #dual of original:
    print("dual of original:")
    dim = W.shape[0]
    v = cp.Variable((dim,1))
    constraints = [W + cp.diag(v) >> 0]
    prob = cp.Problem(cp.Maximize( -cp.sum(v) ),
                      constraints)
    prob.solve()
    
    lower_bound = 0
    if prob.status not in ["infeasible", "unbounded"]:
        print("Optimal value: %s" % prob.value)
        lower_bound = prob.value

    print("lower_bound: ", lower_bound)

    #dual of relaxed:

    #restrict to PSD for randomized sampling
    #on proper covariance matrix later
    X = cp.Variable((dim,dim), PSD=True)
    
    constraints = [X >> 0, cp.diag(X) == np.ones((dim,))]
    prob = cp.Problem(cp.Minimize( cp.trace(cp.matmul(W,X)) ),
                      constraints)
    prob.solve(solver=cp.SCS, max_iters=4000,
               eps=1e-11, warm_start=True)

    print("prob.status:", prob.status)
    
    if prob.status not in ["infeasible", "unbounded"]:
        print("Optimal value: %s" % prob.value)
    
    ret = prob.variables()[0].value

    K = 100 #number of samples
    xs_approx = np.random.multivariate_normal(np.zeros((dim,)),
                                              ret, size=(K))
    xs_approx = np.sign(xs_approx) #shape: (K,dim)

    return xs_approx
    
def greedy(x,W):
    xx = x
    val = (xx.T).dot(W).dot(xx)
    while True:
        idx = None
        for i in range(0, xx.size):
            y = xx
            y[i] = -y[i]
            v = (y.T).dot(W).dot(y)
            if v < val:
                val = v
                idx = i
        if idx is None:
            break
        else:
            xx[i] = -xx[i]
    return val, xx

def solve_d_c(W):
    
    print("performing greedy search")
    dim = W.shape[0]
    best_val = math.inf
    best_x = None
    xs = solve_rand(W)
    
    for i in range(0,xs.shape[0]):
        val, xx = greedy(xs[i,:].T, W)
        if val < best_val:
            best_val = val
            best_x = xx
            
    print("problem size:", W.shape[0])
    print("best objective (randomized+greedy): ", best_val )
    print("-----")
    return best_val, best_x

m = loadmat('../data/hw4data.mat')
w5 = np.array(m['W5'])
w10 = np.array(m['W10'])
w50 = np.array(m['W50'])

solve_d_c(w5)
solve_d_c(w10)
solve_d_c(w50)
\end{verbatim}
    \end{itemize}
    \pagebreak    
  \end{enumerate}
  \item Interior Point Method
\end{enumerate}

\end {document}
